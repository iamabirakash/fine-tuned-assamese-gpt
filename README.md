# ğŸ§  Mini GPT From Scratch

A clean, educational implementation of the **data pipeline and core components required to train a GPT-style language model**, built step-by-step using PyTorch and tokenization tools.

This project focuses on understanding how modern LLMs are constructed under the hood â€” from raw text to model-ready tensors.

---

## ğŸš€ Features

* ğŸ“ CSV text ingestion (Hindi/English compatible)
* ğŸ”¤ GPT-2 BPE tokenization via `tiktoken`
* ğŸ§© Sliding window dataset generation
* ğŸ“¦ PyTorch `Dataset` and `DataLoader`
* ğŸ”¢ Token embedding layer
* ğŸ¯ Language modeling head (logits projection)
* ğŸ§ª Shape and decoding sanity checks
* ğŸ§µ Memory-safe sampling for large datasets

---

## ğŸ—ï¸ Pipeline Overview

```
Raw CSV Text
   â†“
Text Sampling
   â†“
Tokenization (GPT-2 BPE)
   â†“
Sliding Window Dataset
   â†“
Embedding Layer
   â†“
Linear LM Head
   â†“
Next-Token Logits
```

> âš ï¸ Note: Transformer blocks are intentionally excluded to focus on the foundational pipeline.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ SLM.ipynb          # Main notebook
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt   # (optional)
```

---

## ğŸ› ï¸ Installation

```bash
pip install torch pandas tiktoken
```

For Google Colab:

```python
!pip install tiktoken
```

---

## â–¶ï¸ Usage

### 1ï¸âƒ£ Upload dataset (Colab)

```python
from google.colab import files
uploaded = files.upload()
```

### 2ï¸âƒ£ Run the notebook

Execute cells sequentially to:

* load text
* tokenize
* build dataset
* generate embeddings
* produce logits

---

## ğŸ§ª Example Output

* Tokenized sequence lengths
* Input/target alignment
* Embedding tensor shapes
* Vocabulary logits

---

## ğŸ¯ Learning Goals

This project helps you understand:

* How GPT training data is prepared
* Why token shifting is required
* How embeddings map tokens to vectors
* How logits predict the next token
* How PyTorch datasets power LLM training

---

## ğŸ”® Future Improvements

* [ ] Positional embeddings
* [ ] Masked self-attention
* [ ] Transformer block
* [ ] Training loop
* [ ] Hindi-optimized tokenizer
* [ ] Multi-GPU support

---

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss improvements.

---

## ğŸ“œ License

MIT License

---

## â­ Acknowledgment

Built for educational purposes to demystify how GPT-style language models work internally.
